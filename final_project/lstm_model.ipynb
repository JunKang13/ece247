{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, BatchNormalization, MaxPooling2D, Reshape, LSTM, TimeDistributed, Permute\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = init_data(subject=None, verbose=True)\n",
    "X_train_aug, y_train_aug, X_test_aug, y_test_aug, X_valid_aug, y_valid_aug= preprocess_data(X_train, y_train, X_test, y_test, verbose=True)\n",
    "x_train_aug, y_train_aug, x_valid_aug, y_valid_aug, x_test_aug, y_test_aug = load_data(X_train_aug, y_train_aug, X_valid_aug, y_valid_aug, X_test_aug, y_test_aug, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_aug.reshape(x_train_aug.shape[0], x_train_aug.shape[1], x_train_aug.shape[3])\n",
    "x_valid = x_valid_aug.reshape(x_valid_aug.shape[0], x_valid_aug.shape[1], x_valid_aug.shape[3])\n",
    "x_test = x_test_aug.reshape(x_test_aug.shape[0], x_test_aug.shape[1], x_test_aug.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 400\n",
    "features = 22\n",
    "\n",
    "# Define the model\n",
    "lstm_model = Sequential()\n",
    "\n",
    "# Adding an LSTM layer\n",
    "# - units: the dimensionality of the output space\n",
    "# - input_shape: shape of the input (e.g., (timesteps, features))\n",
    "lstm_model.add(LSTM(units=22, return_sequences=True, input_shape=(250, 22), dropout=0.5, recurrent_dropout=0.1))\n",
    "lstm_model.add(BatchNormalization())\n",
    "\n",
    "lstm_model.add(LSTM(units=22, return_sequences=True, input_shape=(250, 22), dropout=0.5, recurrent_dropout=0.1))\n",
    "lstm_model.add(BatchNormalization())\n",
    "\n",
    "# lstm_model.add(LSTM(units=22, return_sequences=True, dropout=0.5, recurrent_dropout=0.1))\n",
    "# lstm_model.add(BatchNormalization())\n",
    "\n",
    "lstm_model.add(Flatten())\n",
    "lstm_model.add(Dropout(0.5))\n",
    "# lstm_model.add(TimeDistributed(Flatten()))\n",
    "# lstm_model.add(TimeDistributed(Dense(16, activation='elu')))\n",
    "lstm_model.add((Dense(4, activation='softmax')))\n",
    "# lstm_model.add(Reshape((-1,32)))\n",
    "# lstm_model.add(TimeDistributed(Dense(units=4, activation='softmax')))  # Change the units based on your problem\n",
    "# lstm_model.add(Flatten())\n",
    "# # Compile the model\n",
    "lstm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=CategoricalCrossentropy(from_logits=True),  # Or another appropriate loss function\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
